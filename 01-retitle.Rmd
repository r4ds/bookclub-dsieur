# Introduction and How to Use this Book

**Learning objectives:**

- Intros & get to know each other
- Understanding community expectations
- Reviewing the reading schedule
- Information on the weekly meetings
- Information on using GitHub

## Slides

`r knitr::include_url("https://docs.google.com/presentation/d/1klx76jh_P277UjQatDzWy7TLrikGW8jvAgHTM0XZp1U/edit?usp=sharing")`

## Meeting Videos

### Cohort 1

`r knitr::include_url("https://www.youtube.com/embed/b1f_Jrxt-Mc")`

<details>
  <summary> Meeting chat log </summary>
  
```
00:03:53    Morgan Grovenburg:  I saw the book mentioned a DataEdu Slack Channel (https://dataedu.slack.com/). Is that still going/active?

00:05:10    Jon Harmon (jonthegeek):    Yes it is! It's not as active as R4DS, but there are usually discussions happening.

00:05:10    Isabella Vel√°squez:    Hey Morgan! That‚Äôs is actually where the book got started :) There are lots of people on that Slack. I can send an invite link to join if anybody is interested.

00:05:36    Morgan Grovenburg:  Yes I'm interested in an invite!

00:05:40    Rob Lucas:  Me too!

00:06:12    Isabella Vel√°squez:    Here it is! Let‚Äôs work together in Slack! It‚Äôs a faster, simpler way to talk shop, share files, and get work done. Join here: https://join.slack.com/t/dataedu/shared_invite/zt-9jkexbww-QxFbexhFoanFicVWaEpyBA

00:16:55    beth.kelly: I prefer the camera too! Sorry.

00:18:23    Isabella Vel√°squez:    Thanks Beth! Same here, I often feel siloed and I can‚Äôt wait to learn from all your work.

00:21:10    Jon Harmon (jonthegeek):    r4ds.io/diseur will take you to the bookdown side of the repo, and allow you to hop over to the GitHub (assuming I set it up correctly)

00:22:04    Layla Bouzoubaa:    John I get a 404

00:22:15    Alyssa Ibarra:  Same here

00:22:47    Ryan Woodbury:  https://github.com/r4ds/bookclub-dsieur

00:23:19    Jon Harmon (jonthegeek):    Hmmm. Evidently it requires https://r4ds.io/dsieur

00:23:35    Jon Harmon (jonthegeek):    ...which might have just solved a bug I've had elsewhere.

00:24:04    Layla Bouzoubaa:    sweeet

00:28:08    beth.kelly: Jon is freezing for everyone else, right?

00:28:12    Morgan Grovenburg:  Yes

00:28:12    Alyssa Ibarra:  I think you‚Äôre freezing, Jon

00:28:17    Daniel Jin: Y

00:28:51    Mark LaVenia:   https://github.com/r4ds/bookclub-dsieur

00:29:52    Daniel Jin: 50% this time tho!

00:30:16    Jon Harmon (jonthegeek):    I will talk about this on the Slack evidently üôÉ

00:30:28    Isabella Vel√°squez:    yeah absolutely. Can‚Äôt wait to get any feedback!!

00:33:12    Jon Harmon (jonthegeek):    My internet is really bad today, gonna drop off!

00:33:28    Ryan Woodbury:  Thanks for setting this up! See you soon.

00:37:28    Isabella Vel√°squez:    Yes! And we‚Äôll send a Slack message once we‚Äôre done.

00:39:46    Ryan Woodbury:  I listened to a podcast about the Kentucky stats project. It sounded way cool!

00:41:44    beth.kelly: That's great! I'd love to hear about the podcast.

00:42:19    beth.kelly: I love the desks in your background Rob!

00:43:14    Rob Lucas:  Thanks, Beth! They came out of a school in Ohio where my grandfather was a principal.

00:43:28    Isabella Vel√°squez:    So glad to hear it!!

00:46:12    Michael H:  Ryan E‚Äôs podcast About Practice is great

00:46:39    Ryan Woodbury:  ^^ Yes ^^

00:49:38    beth.kelly: So interested in retention in higher ed!

00:51:28    Ryan Woodbury:  Here is the podcast link the ‚ÄúData Ed Chat‚Äù, episode 10 discusses the Kentucky Stats Explorer: https://www.buzzsprout.com/1074286

00:51:45    Ryan Woodbury:  And the ‚ÄúAbout Practice‚Äù podcast with two of the books authors: https://anchor.fm/about-practice/episodes/1-Hypothesis-Testing-for-Kids-is-Basically-Torture-eq0qc6

00:53:48    Morgan Grovenburg:  I think that was everyone

00:54:30    Mark LaVenia:   Thank you! :)

00:54:55    Daniel Jin: Thank you Ryan and Isabella for organizing! Can't wait!

00:55:00    Mark LaVenia:   I'll stick around for that too :)

00:55:13    Marina: This is my first time participating in something like this. Usually, how long do these sessions last?

00:56:01    Ryan Woodbury:  https://www.youtube.com/watch?v=d41oc2OMAuI&feature=youtu.be

00:56:49    Ryan Woodbury:  The sessions are blocked for an hour. However, I have seen some videos from other book clubs that last for 30 minutes. It‚Äôll depend on the week.

00:57:31    Marina: Ok, thanks!

01:08:02    Mark LaVenia:   very helpful. thanks!
```
</details>

# What does Data Science in Education Look Like?

**Learning objectives:**

- TBD

## Slides

- `r knitr::include_url("https://docs.google.com/presentation/d/1FhmztqUQQotG3tOXrq96mlaiFF4NfjXMpx43Q9mf8pQ/edit?usp=sharing")`

## Meeting Videos

### Cohort 1

TBD
`r knitr::include_url("https://www.youtube.com/embed/b1f_Jrxt-Mc")`

<details>
  <summary> Meeting chat log </summary>
  
```
00:12:11	Rob Lucas:	PollEv.com/roblucas198
00:14:20	Daniel Jin:	sorry have to step away for a moment!
00:21:24	Marina:	I don't currently work in education so I'm going to skip this question (:
00:32:17	Mark LaVenia:	data curation is a big part of my role
00:32:41	Ronak Patel:	I have to hop off for a work call but will make sure to carve out an hour here on out. Thanks all!
00:32:47	Alyssa Ibarra:	what is data curation? Sorry not familiar with the term
00:34:00	Mark LaVenia:	@Alyssa - Finding data that teams need to answer a question, etc.
00:34:16	Alyssa Ibarra:	Ah yes, thanks!
00:39:58	Mark LaVenia:	data interrogation
00:42:15	Mark LaVenia:	need to, but don't
00:42:20	Ryan Woodbury:	same
00:42:27	Marina:	^^^
00:56:18	Marina:	I know this is an R-focused thing, but for basic/quick spatial analysis, I've really enjoyed using QGIS, which is more point and click
00:57:41	Mark LaVenia:	thanks @Marina. Really trying to stretch into R and Python. Thanks for the tip on QGIS; will check that out
01:02:57	Mark LaVenia:	+1
01:03:52	Mark LaVenia:	Nicely done Rob
```
</details>

# Special Considerations

**Learning objectives:**

- Understand data science and change management
- Discuss reproducibility in our work
- Share resources

## Slides

`r knitr::include_url("https://docs.google.com/presentation/d/1wIdhLRDrO4hyFVwulRD_YxECZk8OkcJQPqk4hM4f0rw/edit")`

### Resources Shared

- Switch: How to Change Things When Change is Hard by Chip Heath, Dan Heath
- [ADKAR Model](https://www.prosci.com/)
- [Education Data Done Right](https://www.eddatadoneright.com/blog/)
- Weapons of Math Destruction by Cathy O'Neil
- Thinking in Systems by Donella H. Meadows 
- [We All Count](https://weallcount.com/)
- [Urban Institute Applying Racial Equity Awareness in Data Visualization](https://urban-institute.medium.com/applying-racial-equity-awareness-in-data-visualization-bd359bf7a7ff)
- [Education Equity Indicator Systems](https://www.nap.edu/catalog/25833/building-educational-equity-indicator-systems-a-guidebook-for-states-and)

## Meeting Videos

### Cohort 1

`r knitr::include_url("https://www.youtube.com/embed/LYve96mqY44")`

<details>
  <summary> Meeting chat log </summary>
  
```
00:10:51	Isabella Vel√°squez:	Here's the Github for the bookclub! https://github.com/r4ds/bookclub-dsieur
00:11:02	Catherine Miller:	thank you!
00:25:22	Catherine Miller:	5 levels, maybe? instructors, aides, administrators, executives, data folks
00:26:17	Rob Lucas (he/him):	5 seems about right, depending where you divide them up
00:27:02	Yukie Toyama:	Yeah 5 groups: researchers, funding organization, teachers, students, and parents/public
00:27:04	Isabella Vel√°squez:	We cut across different strategies on our team. Top ones are leadership, non-technical team members, technical team members, external partners
00:27:37	Catherine Miller:	the technical and non-technical categories are useful for this
00:28:00	Catherine Miller:	plus external partners who see a different grain size of data
00:28:19	Mark LaVenia:	We have an Executive Director, Chiefs, Directors, Specialists, and Managers--and my work requires working across and between all levels
00:28:47	Isabella Vel√°squez:	yes. data privacy concerns are really important when considering what data is being shared
00:28:53	Catherine Miller:	Thinking in Systems is also useful? Donella Meadows
00:32:39	Catherine Miller:	that is cool
00:42:23	Catherine Miller:	https://www.eddatadoneright.com/blog/
00:45:02	Isabella Vel√°squez:	A couple of resources I'm digging into:
00:45:17	Isabella Vel√°squez:	Urban Institute Applying Racial Equity Awareness in Data Visualization https://urban-institute.medium.com/applying-racial-equity-awareness-in-data-visualization-bd359bf7a7ff
00:45:45	Isabella Vel√°squez:	National Academies of Sciences, Engineering, and Medicine https://www.nap.edu/catalog/25833/building-educational-equity-indicator-systems-a-guidebook-for-states-and
00:47:05	Edgar Zamora:	https://weallcount.com
00:50:20	Isabella Vel√°squez:	great points!!
00:55:44	Mark LaVenia:	Thank you!
```
</details>

# Getting Started with R and RStudio & Foundational Skills

**Learning objectives:**

- Knitting ppt slides from Rmd
- Writing function
- Reading/nterpretting code
- Avoiding/debugging typos
- Piping `%>%` and convention preference `<-` vs `->`

## Slides

`r knitr::include_url("./R/dsieur_ch5n6.pptx")`

<details>
  <summary> Foundational Skills Script </summary>
```  
`# using the filter() function from the stats package  
x <- 1:100  

stats::filter(x, rep(1, 3))  

`# using the filter() function from the dplyr package  
starwars %>%   
  dplyr::filter(mass > 85)

dataedu::ma_data_init

dataedu::ma_data_init -> ma_data

ma_data_init <- dataedu::ma_data_init

# you probably wrote these 3 library() lines in your R script file earlier
# if you have not yet run them, you will need to run these three lines before running the rest of the chunk
library(tidyverse)
library(dataedu)
library(skimr)
library(janitor)

# Exploring and manipulating your data
names(ma_data_init)

glimpse(ma_dat_init) 

glimpse(ma_data_init)

summary(ma_data_init)

glimpse(ma_data_init$Town)

summary(ma_data_init$Town)

glimpse(ma_data_init$AP_Test Takers)

glimpse(ma_data_init$`AP_Test Takers`)

summary(ma_data_init$`AP_Test Takers`)


ma_data_init %>% 
  group_by(District Name) %>%  
  count()

ma_data_init %>% 
  group_by(`District Name`) %>% 
  count()  

ma_data_init %>% 
  group_by(`District Name`) %>% 
  count() %>% 
  filter(n > 10)

ma_data_init %>% 
  group_by(`District Name`) %>% 
  count() %>% 
  filter(n > 10) %>% 
  arrange()
```
</details>

## Meeting Videos

### Cohort 1

`r knitr::include_url("https://www.youtube.com/embed/QzctSsk2ok0")`

<details>
  <summary> Meeting chat log </summary>
  
```
00:06:21    Layla Bouzoubaa:    Hello everyone
00:06:47    Catherine Miller:   Hello!
00:07:29    Mark LaVenia:   Hi everyone
00:13:40    Ronak Patel:    3 minutes in and I've already learned something new!
00:13:51    Daniel Jin: +1
00:13:51    Morgan Grovenburg:  Me too!
00:14:53    Ryan Woodbury:  It looks way better in the book down compared to Google Slides
00:14:56    Carlo M:    reproducibility
00:16:17    Catherine Miller:   thanks, that was helpful
00:17:25    Mike Haugen:    The R markdown chapter in R4DS goes over benefits: https://r4ds.had.co.nz/r-markdown.html
00:22:30    shamsuddeen:    New approach for installing packages: https://github.com/r-lib/pak
00:26:20    Ryan Woodbury:  YES!!! I just used it!
00:26:36    Carlo M:    Syntactic sugar of ifelse(is.na(certification), certification_1, certification)
00:27:04    Marina: ^^ yeah, I had been using ifelse, but this looks neater
00:27:05    Ryan Woodbury:  Trying to combine cohort data where each cohort has different variable names.
00:30:42    Alyssa Ibarra:  According to Help, coalesce is from dplyr
00:30:55    Layla Bouzoubaa:    janitor
00:30:59    shamsuddeen:    coalesce is from dplyr?
00:31:09    Layla Bouzoubaa:    janitor::remove_empty()
00:31:14    shamsuddeen:    Oh yah, I see it
00:31:25    Carlo M:    Yup, coalesce is dplyr
00:37:22    Layla Bouzoubaa:    If you do in the ?stats::filter console, it is for time series data
00:37:32    Layla Bouzoubaa:    He‚Äôs correct
00:37:43    Alyssa Ibarra:  Thanks for that clarification!
00:39:55    Morgan Grovenburg:  Always <-
00:40:00    Mike Haugen:    ^^
00:40:14    Ronak Patel:    -> gives me anxiety
00:40:28    Carlo M:    When I get lazy, I end up using `->`
00:40:43    Morgan Grovenburg:  My reason is to keep = straight
00:40:48    Isabella Vel√°squez:    same! I only use -> when I don‚Äôt want to scroll all the way to the beginning of a pipe XD
00:40:54    Layla Bouzoubaa:    Second Ronan‚Äôs point
00:41:01    Layla Bouzoubaa:    ronak*
00:42:53    Isabella Vel√°squez:    I once spent a lot of time before realizing I used ‚Äò instead of `
00:46:40    Carlo M:    Morgan is right. The {stats} library would get masked when library(dplyr) is called
00:50:07    shamsuddeen:    Happy Git with R is a good book on learning Gihub: https://happygitwithr.com
00:52:56    Layla Bouzoubaa:    Sorry everyone I need to hop off. Cheers!
00:53:01    Mike Haugen:    How do you get a branch?
00:54:53    Alyssa Ibarra:  ^ same question
00:56:29    Catherine Miller:   Thank you Mark :)
00:56:31    Marina: This discussion is giving me scary git flashbacks
00:56:39    Catherine Miller:   githell, right?
00:57:20    Marina: ^^^
00:58:53    Carlo M:    You‚Äôre doing great!!
00:59:16    Catherine Miller:   I always get lost here, too.
01:02:06    Isabella Vel√°squez:    I always create a branch by clicking the button next to main in R Studio
01:03:48    Ronak Patel:    I gotta hop off for another meeting. Thanks, Mark. Great job today! See ya'll next week.
01:03:49    Catherine Miller:   Thank you!!
01:04:18    Catherine Miller:   Thank you for being brave and helping us learn, Mark!!
01:04:20    shamsuddeen:    Thanks
01:04:24    Alyssa Ibarra:  Thank you!
01:04:37    Carlo M:    Thanks Mark!
```
</details>

# Walkthrough 1: The Education Data Science Pipeline 

**Learning objectives:**

Learning about key steps in data preparation, processing, wrangling using tidyverse (a set of packages).

## Slides

`r knitr::include_url("https://docs.google.com/presentation/d/1sJgXd8weNnuCDTYFdb3iWErls0cVIefXbCegVBI3zTw/edit")`

<details>
  <summary> Walkthrough 1 Script </summary>
```{r, eval = FALSE}
# useful shortcut#
  # Run current line/selection: cmd + return (MAC), ctrl + Enter (Windows)
  # Assignment sign (<-) : Option + - (M), Alt + - (W)
  # Pipe sign (%) : cmd + shift + M (M), ctrl + shift + M (W)

install.packages(c("tidyverse", "apaTables", "sjPlot", "dataedu", "summarytools", "ggpubr"))

# dataedu wasn't available for R ver 3.6.3 so I installed dev version of dataedu
remotes::install_github("data-edu/dataedu")

# this was not in the book but useful to get descriptives
install.packages("summarytools")

# Load packages
library(tidyverse)
library(apaTables)
library(sjPlot)
library(readxl)
library(dataedu)
library(summarytools)
library(ggpubr)

############################
# import data from dataedu #
############################

# Pre-survey for the F15 and S16 semesters
pre_survey <- dataedu::pre_survey

# Gradebook and log-trace data for F15 and S16 semesters
course_data <- dataedu::course_data

# Log-trace data for F15 and S16 semesters - this is for time spent
course_minutes <- dataedu::course_minutes

#############
# view data #
#############
pre_survey
head(pre_survey) # first six rows
View(pre_survey) # a full view in a separate tab
glimpse(pre_survey) # list of variables, values for the first couple of cases

# get a quick look at each variable in df
view(dfSummary(pre_survey)) 

## take a look at course_data, course_minutes; what do you notice?

########################
# 1.process pre_survey #
########################
pre_survey  <-
  pre_survey  %>%
  # Rename the qustions something easier to work with because R is case sensitive
  # and working with variable names in mix case is prone to error
  rename(
    q1 = Q1MaincellgroupRow1,
    q2 = Q1MaincellgroupRow2,
    q3 = Q1MaincellgroupRow3,
    q4 = Q1MaincellgroupRow4,
    q5 = Q1MaincellgroupRow5,
    q6 = Q1MaincellgroupRow6,
    q7 = Q1MaincellgroupRow7,
    q8 = Q1MaincellgroupRow8,
    q9 = Q1MaincellgroupRow9,
    q10 = Q1MaincellgroupRow10
  ) %>%
  # Convert all question responses to numeric
  mutate_at(vars(q1:q10), list( ~ as.numeric(.))) # q1-10 are already numeric, so this doesn't seem necessary

# you could insert suffix to the var names indicate the three dimensions
# e.g. q1.i, q2.uv, 3.pc where i = interest, u = utility value, p = perceived competence

#############################################
#1a.practice mutate = making a new variable #
#############################################
# create a df (dataframe) in tibble format with two columns/vars: male & female
df <- tibble(
  male = 5, 
  female = 5
)

# Use mutate to create a new column called "total_students" 
# populate that column with the sum of the "male" and "female" variables
df %>% mutate(total_students = male + female)

# let's keep this new column in df
df <- df %>%  mutate(total_students = male + female)

######################################################
# 1b. reverse_score function with mutate & case_when #
######################################################
# This part of the code is where we write the function:
# Function for reversing scales 
reverse_scale <- function(question) {
  # Reverses the response scales for consistency
  #   Arguments:
  #     question - survey question
  #   Returns: 
  #    a numeric converted response
  # Note: even though 3 is not transformed, case_when expects a match for all
  # possible conditions, so it's best practice to label each possible input
  # and use TRUE ~ as the final statement returning NA for unexpected inputs
  x <- case_when(
    question == 1 ~ 5,
    question == 2 ~ 4,
    question == 3 ~ 3, 
    question == 4 ~ 2,
    question == 5 ~ 1,
    TRUE ~ NA_real_
  )
  x
}

# let's see how it works
reverse_scale(pre_survey$q4)
pre_survey$q4 # compare with original

# And here's where we use that function to reverse the scales
# We use the pipe operator %>% here
# Reverse scale for questions 4 and 7
pre_survey <-
  pre_survey %>%
  mutate(q4 = reverse_scale(q4), # mutate with the original var name to overwrite
         q7 = reverse_scale(q7))

# Note: psych package has reverse.code() function so you don't have to write your own

#####################################################
#1c. pivot_longer to make pre_survey into long form #
#####################################################

# Pivot the dataset from wide to long format
# And name the long format df as measure_mean
measure_mean <-
  pre_survey %>%
  # Gather questions and responses
  pivot_longer(cols = q1:q10,
               names_to = "question", # give a new var/col name "question" where question # will go
               values_to = "response") # give a new var/col name "response" where response values will go

# create a new var called measure to denote 3 dimensions of motivation 
measure_mean <- measure_mean %>% 
  # Here's where we make the column of question categories called "measure"
  mutate(
    measure = case_when(
      question %in% c("q1", "q4", "q5", "q8", "q10") ~ "int",
      question %in% c("q2", "q6", "q9") ~ "uv",
      question %in% c("q3", "q7") ~ "pc",
      TRUE ~ NA_character_)
  )

###################################################
# 1d. Get mean scores for each motivation measure #
# Across ~912 students who responded the pre-survey
# using group_by() and summarize()
###################################################

measure_mean <- measure_mean %>%
  # First, we group by the new variable "measure"
  group_by(measure) %>%
  # Here's where we compute the mean of the responses
  summarize(
    # Creating a new variable to indicate the mean response for each measure
    mean_response = mean(response, na.rm = TRUE),
    # Creating a new variable to indicate the percent of each measure that 
    # had NAs in the response field
    percent_NA = mean(is.na(response))
  )

measure_mean

##############################
# 2. Process the course data #
##############################

View(course_data)

# split course section into components
course_data <- 
  course_data %>%
  # Give course subject, semester, and section their own columns
  separate(
    col = CourseSectionOrigID,
    into = c("subject", "semester", "section"),
    sep = "-",
    remove = FALSE # this is to keep the original var
  )

#############################################
# 3. Join/merge course_data with pre_survey #
#############################################

#rename pre_survey id vars
pre_survey <-
  pre_survey %>%
  rename(student_id = opdata_username, #new_var_name = old_var_name
         course_id = opdata_CourseID)

pre_survey

################################################
#3a.  extract 5 digits inbetween _ _ in student_id #
################################################

#trying str_sub just with one string value
str_sub("_99888_1", start = 2)
str_sub("_99888_1", start = -3)
str_sub("_99888_1", start = 2, end = -3)

# Re-create the variable "student_id" so that it excludes the extraneous characters
pre_survey <- pre_survey %>% 
  mutate(student_id = str_sub(student_id, start = 2, end = -3))

# Save the new variable as numeric so that R no longer thinks it is text 
pre_survey <- pre_survey %>% 
  mutate(student_id = as.numeric(student_id))

###########################################################
#3b rename id vars in course_data and join with pre_survey
##########################################################
course_data <-
  course_data %>%
  rename(student_id = Bb_UserPK,
         course_id = CourseSectionOrigID)

# new df merges course_data with pre_survey
dat <-
  left_join(course_data, pre_survey,
            by = c("student_id", "course_id"))
dat

############################################
#4. Process course_minutes & join with dat
############################################
course_minutes <-
  course_minutes %>%
  rename(student_id = Bb_UserPK,
         course_id = CourseSectionOrigID)

course_minutes <-
  course_minutes %>%
  # Change the data type for student_id in course_minutes so we can match to 
  # student_id in dat
  mutate(student_id = as.integer(student_id))

dat <- 
  dat %>% 
  left_join(course_minutes, 
            by = c("student_id", "course_id"))

# dat has many gradebook_items per student per course
# we want just one row per student & course combo
# using distinct()
dat <-
  distinct(dat, course_id, student_id, .keep_all = TRUE)

# rename final grade var
dat <- rename(dat, final_grade = FinalGradeCEMS)

###########################################
# 5. Analysis
###########################################

#######################################################################
# 5a.Scatter plot to examine relationship between final grade & time spent
#######################################################################
view(dfSummary(dat)) 

#scatter plot to see relationship between timespent & final grade
p1 <- dat %>%
  # aes() tells ggplot2 what variables to map to what feature of a plot
  # Here we map variables to the x- and y-axis
  ggplot(aes(x = TimeSpent, y = final_grade)) + 
  # Creates a point with x- and y-axis coordinates specified above
  geom_point(color = dataedu_colors("green")) + 
  theme_dataedu() +
  labs(x = "Time Spent",
       y = "Final Grade")

# add a line of best fit
p1 +  geom_smooth(method = "lm")

# with ggpubr, you can add correlation to the graph
require(ggpubr)
p2 <- ggscatter(dat, x = "TimeSpent", y = "final_grade",
                color = "springgreen4",
                add = "reg.line",  # Add regressin line
                add.params = list(color = "blue", fill = "lightgray"), # Customize reg. line
                conf.int = TRUE # Add confidence interval
                )
# Add correlation coefficient
p2 + 
  stat_cor(method = "pearson", label.x = 3900, label.y = 130,
           p.accuracy = 0.001, r.accuracy = 0.01)

###################################################
# 5b.Linear regression with time spent as predictor
###################################################
m_linear <-
  lm(final_grade ~ TimeSpent, data = dat)

summary(m_linear)

# get publication ready table with tab_model function
require(sjPlot)
tab_model(m_linear,
          title = "Table 7.1")
# you can copy and paste it into Word!

# or save it with apa.re.table function
apa.reg.table(m_linear, filename = "regression-table-output.doc")

###################################################
# 5c.Correlations among the 3 motivation variables
###################################################

# pivot survey_responses to long form
survey_responses <-
  pre_survey %>%
  # Gather questions and responses
  pivot_longer(cols = q1:q10,
               names_to = "question",
               values_to = "response") %>%
  mutate(
    # Here's where we make the column of question categories
    measure = case_when(
      question %in% c("q1", "q4", "q5", "q8", "q10") ~ "int",
      question %in% c("q2", "q6", "q9") ~ "uv",
      question %in% c("q3", "q7") ~ "pc",
      TRUE ~ NA_character_
    )) 

# create mean_response for each student for each measure
survey_responses <-
  survey_responses %>%
  group_by(student_id, measure) %>%
  # Here's where we compute the mean of the responses for each stdt & measure combo
  summarize(
    # Mean response for each measure
    mean_response = mean(response, na.rm = TRUE)
  ) 

# Filter NA (missing) responses and pivot to wide form
survey_responses <-
  survey_responses %>%
  filter(!is.na(mean_response)) %>%
  pivot_wider(names_from = measure, 
              values_from = mean_response)
survey_responses

# get correlation table
survey_responses %>%
  apa.cor.table(filename = "corr-table-output.doc")
# note the correlation table includes student_id. 
# probably want to delete it in Word

#############################################################################
# 5d. Linear regression with hours sptent (rather than minutes) as predictor
############################################################################

# creating a new variable for the amount of time spent in hours
dat <- 
  dat %>% 
  mutate(TimeSpent_hours = TimeSpent / 60)

# the same linear model as above, but with the TimeSpent variable in hours
m_linear_1 <- 
  lm(final_grade ~ TimeSpent_hours, data = dat)

# viewing the output of the linear model
tab_model(m_linear_1,
          title = "Table 7.2")

##################################################################
# 5e. Linear regression with standardized time spent as predictor
##################################################################
# this is to standardize the TimeSpent variable to have a mean of 0 and a standard deviation of 1
# this makes intercept more interpretable
dat <- 
  dat %>% 
  mutate(TimeSpent_std = scale(TimeSpent))

# the same linear model as above, but with the TimeSpent variable standardized
m_linear_2 <- 
  lm(final_grade ~ TimeSpent_std, data = dat)

# viewing the output of the linear model
tab_model(m_linear_2,
          title = "Table 7.3")

#####################################################################
#6. Multiple regression model with time spent & subject as predictors
#####################################################################
# a linear model with the subject added 
# independent variables, such as TimeSpent_std and subject, can simply be separated with a plus symbol:
m_linear_3 <- 
  lm(final_grade ~ TimeSpent_std + subject, data = dat)
# note: subject is a categorical variable and it seems AnPhA (animal physiology)
# is set as the reference category (numbers assinged by alphabetical order)

tab_model(m_linear_3,
          title = "Table 7.4")

# Combine all four models in one table & show standard errors rather than CIs
tab_model(m_linear, m_linear_1, m_linear_2, m_linear_3,
          show.ci = FALSE, show.se = TRUE)

#####################################################################
#7. What other analyses can you think of?
#####################################################################

# Add total scores of pre-course motivation as a predictor?
# --> use mutate to create sum_motiv variable

# Does the effect of time spent vary by subjects/courses?
# --> add time x subject interaction term
# Maybe color dots in scatter plot by subject to see if there is a pattern

ggplot(data = dat, 
       aes(x = TimeSpent, y = final_grade, color = subject)) + 
  geom_point() + 
  theme_dataedu() +
  labs(x = "Time Spent",
       y = "Final Grade")
```
</details>

## Meeting Videos

### Cohort 1

`r knitr::include_url("https://www.youtube.com/embed/lXWAfm0fh7Q")`

<details>
  <summary> Meeting chat log </summary>
  
```
00:04:35	Ryan Woodbury:	https://rfordatascience.slack.com/files/UQ4DR12BY/F01QUFD8V5H/dsieur_ch7_slides
00:04:50	Ryan Woodbury:	https://rfordatascience.slack.com/files/UQ4DR12BY/F01RJ4ENF4Y/desieur_ch7_scripts.r
00:04:55	Ryan Woodbury:	Slides, then script
00:10:33	Ryan Woodbury:	Is it like skimr?
00:14:57	Isabella Vel√°squez:	super clear! love the color coding
00:16:26	Edgar Zamora:	https://www.garrickadenbuie.com/project/tidyexplain/ I use these GIFs to help me visualize the different kind of joins. Get confused
00:17:31	Rob Lucas:	Thanks for sharing that Edgar! The semi and anti joins were new to me. I think this will help me visualize them.
00:26:26	Mark LaVenia:	you are doing great! Thanks!
00:29:29	Ryan Woodbury:	The `mutate_*()` functions are being superseded by using the `across()` function within `mutate()`.
00:30:03	Isabella Vel√°squez:	here's some documentation on the mutate_* functions. https://dplyr.tidyverse.org/reference/mutate_all.html but like Ryan said, they've been superseded. I am still learning across()!
00:30:05	Ryan Woodbury:	Line 69 would be: mutate(across(q1:q10, as.numeric)) in the "new" format
00:30:37	Alyssa Ibarra:	Thanks, Ryan! I didn't know it was changing
00:30:49	Ryan Woodbury:	I was just getting used to the mutate_*() functions too!
00:37:57	Alyssa Ibarra:	when would you use mutate versus transmute?
00:38:35	Ryan Woodbury:	The issue with the psych::reverse.code() function that Yukie is talking about is that it is not tidyverse friendly, *but* is a function that is already made and does a great job. (I love the psych package, BTW.)
00:45:41	Mark LaVenia:	I love that
01:00:27	Ryan Woodbury:	Great advice on imagining the joined datasets.
01:01:19	Isabella Vel√°squez:	I've got to hop off at 5. Thank you SO much Yukie! That was fantastic !
01:01:39	Ryan Woodbury:	Thank you! Great work.
01:02:58	Mark LaVenia:	Great job Yukie!
01:03:00	Alyssa Ibarra:	Thank you so much! It was so great!
01:03:09	Edgar Zamora:	Great job! Thank you!
```
</details>

# Walkthrough 2: Approaching Gradebook Data From a Data Science Perspective

**Learning objectives:**

Learn about importing excel spreadsheets using a file path or `here()`, tidying data with `clean_names()` and `remove_empty()` from the {janitor} package, transforming data with `pivot_longer()` using `contains()` from the {stringr} package, visualizing data, correlation analysis with `cor()`, and modeling data with `lm()`.

## Slides

`r knitr::include_url("./R/2021-03-17/chapter_8_presentation.html")`

<details>
  <summary> Walkthrough 2 Rmd Script </summary>

`r knitr::include_url("./R/2021-03-17/chapter_8_notes.html")`

</details>

## Meeting Videos

### Cohort 1

`r knitr::include_url("https://www.youtube.com/embed/CNRWp-ao_Pc")`

<details>
  <summary> Meeting chat log </summary>
  
```
00:24:07	Rob Lucas:	Do you all use {janitor}? The book mentioned that it was developed by an education person, and I wondered if it was used more in education contexts.
00:24:23	Ronak Patel:	clean_names() has changed my life
00:24:26	Alyssa Ibarra:	I have never used it before this.
00:24:39	edgarzamora:	^^^ Ronak +1
00:24:45	Isabella Vel√°squez:	+1 to clean_names()!
00:25:55	Isabella Vel√°squez:	{janitor} is by Sam Firke from The New Teacher Project, but I think can be used in any data cleaning context : https://github.com/sfirke/janitor
00:26:34	Mike Haugen:	And then when you create objects, do you make them snake_case?
00:26:38	Mike Haugen:	https://r4ds.had.co.nz/workflow-basics.html?q=snake_case#whats-in-a-name
00:27:32	Isabella Vel√°squez:	usually I do. I only use camelCase in Shiny
00:27:50	edgarzamora:	By default it is snake_case
00:29:56	edgarzamora:	select(-c(absent, late))
00:30:30	Ryan Woodbury:	I do it that way too
00:30:54	Ronak Patel:	same, especially when exploring data. never know what you want to drop and it saves some time.
00:31:18	Isabella Vel√°squez:	are these xaringan slides? so cool
00:37:38	Rob Lucas:	Why color code those if the boxplots aren't even ordered correctly?
00:38:06	Ronak Patel:	does anyone have a quick way to order them?
00:38:34	edgarzamora:	fct_reorder() maybe?
00:38:58	Ryan Woodbury:	Check out, reorder_within() from tidytext
00:40:03	Ryan Woodbury:	https://juliasilge.com/blog/reorder-within/
00:50:54	Isabella Vel√°squez:	Awesome job!!!
00:53:38	Ronak Patel:	in a regression, isn't independence of variables an assumption?
00:54:03	Ryan Woodbury:	The errors are independent
00:54:06	Ryan Woodbury:	Should be
00:55:10	Ryan Woodbury:	That is a good question. I want to help find an answer!
00:55:46	Mike Haugen:	Thanks!
```
</details>

# Walkthrough 3: Using School-Level Aggregate Data to Illuminate Educational Inequities

**Learning objectives:**

This chapter explores what aggregate data is, and how to access, clean, and explore it. 

## Slides

`r knitr::include_url("./R/2021-03-17/Ch9_walkthrough3.html")`

## Meeting Videos

### Cohort 1

`r knitr::include_url("https://www.youtube.com/embed/Z5KRaOgW0sk")`

<details>
  <summary> Meeting chat log </summary>
  
```
00:26:06	Ryan Woodbury:	Here's a bit of history and potential future directions for FRL metric: https://dataqualitycampaign.org/resource/accurate-student-poverty-data-is-crucial-to-supporting-all-students/
00:30:14	Isabella Vel√°squez:	here's another reference on FRPL; Urban Institute is coming up with an alternative measure (disclosure: my coworker is funding this project): https://www.urban.org/features/measuring-student-poverty-dishing-alternatives-free-and-reduced-price-lunch
00:31:39	Isabella Vel√°squez:	also want to share my favorite tweet :) https://twitter.com/andrewheiss/status/1021944992351186944?s=21
00:36:34	Ronak Patel:	Thanks for sharing those articles!
```
</details>

# Walkthrough 4: Longitudinal Analysis With Federal Students With Disabilities Data

**Learning objectives:**

* Longitudinal analysis uses
* Importing data options
* Processing considerations: what order to take, what to consider for NA's
* Importance of visualizing data
* Considering next steps for modeling analysis

## Slides

`r knitr::include_url("./R/2021-03-31/Walkthrough-4.html")`

## Meeting Videos

### Cohort 1

`r knitr::include_url("https://www.youtube.com/embed/6k_F6XGEmqs")`

<details>
  <summary> Meeting chat log </summary>
  
```
00:41:00	edgar zamora:	https://stackoverflow.com/questions/4862178/remove-rows-with-all-or-some-nas-missing-values-in-data-frame
00:41:14	Mike Haugen:	For time series analysis, some of the forecasting functions, e..g exponential smoothing, require a certain approach to dealing with NAs. You can remove NAs for some, for others, you need to impute them
00:42:20	Arami:	Can you explain what "time series analysis" is? Is it any analysis that tracks change over time?
00:43:09	Mike Haugen:	Yes
00:43:48	Mike Haugen:	Like forecasting emergency room presentations based on historical data on emergency room presentations over the last few years
00:44:07	Mike Haugen:	or forecasting course attendance based on historical data.
00:45:18	Rob Lucas:	Glad to know there is some other list-aversion out there!
00:45:27	Mike Haugen:	For R, see Hyndman Forecasting: Principles and Practice: https://otexts.com/fpp3/index.html
00:45:46	Arami:	Thanks!
00:45:47	Ronak Patel:	I also suffer from severe list-aversion.
00:46:33	Morgan Grovenburg:	injuries %>%
  mutate(diag = fct_lump(fct_infreq(diag), n = 5)) %>%
  group_by(diag) %>%
  summarise(n = as.integer(sum(weight)))
```
</details>

# Walkthrough 5: Text Analysis With Social Media Data

**Learning objectives:**

+ Understand how to retrieve data from Twitter
+ Understand the robustness of Twitter data
+ Learn basic principles of **N**atural **L**anguage **P**rocessing (NLP)
+ Learn how to apply those principles to process social media data
+ Learn how to use the `tidytext` package to analyze social media data
+ Understand what a *sentiment analysis* is
+ Learn how to perform a *sentiment analysis* on Twitter data

## Slides

`r knitr::include_url("./R/2021-04-07/walkthrough-5.html")`

## Meeting Videos

### Cohort 1

`r knitr::include_url("https://www.youtube.com/embed/yt0PNLeo80o")`

<details>
  <summary> Meeting chat log </summary>
  
```
00:07:35	Layla Bouzoubaa:	Lol I love the office band, I have it too!!
00:07:58	Ronak Patel:	Always good for a chuckle at the start of a meeting!
00:32:49	Morgan Grovenburg:	If you haven't used `tidytext` before, I recommend Julia Silge's free learnr course: https://juliasilge.com/blog/learn-tidytext-learnr/
00:33:18	Alyssa Ibarra:	Thanks! Is it just using whitespace as delimiter?
00:33:40	shamsuddeen:	Also the book : https://www.tidytextmining.com
00:43:08	Mike Haugen:	Love the Tidy Text book. Julia Silge also has Supervised Machine Learning for Text Analysis in R: https://smltar.com/
01:02:30	Alyssa Ibarra:	Thank you!
01:03:46	shamsuddeen:	Good talk
01:04:35	Louis Carlo Medina:	thank you!!
```
</details>


# Walkthrough 6: Exploring Relationships Using Social Network Analysis With Social Media Data

**Learning objectives:**

+
+
+

## Slides

`r knitr::include_url("./R/2021-04-14/walkthrough-6.html")`

## Meeting Videos

### Cohort 1

`r knitr::include_url("https://www.youtube.com/embed/pHcan7NOXRs")`


<details>
  <summary> Meeting chat log </summary>
  
```
00:15:11	Ryan Woodbury:	I‚Äôm convinced!
00:23:20	Isabella Vel√°squez:	I remember igraph <3
00:32:57	Isabella Vel√°squez:	#rstats ?
00:35:19	Ronak Patel:	Sorry guys, I have to hop off to deal with something. Thanks for the presentation Carlo! Gonna go research network graphs.
00:35:35	Isabella Vel√°squez:	Does anybody have other packages for visualizing network graphs?
00:37:00	Alyssa Ibarra:	I‚Äôve used visNetwork a couple of years ago, but it was a one off project and I forgot it all. :(
00:40:46	Ryan Woodbury:	I‚Äôm only slightly familiar with non-R software, Neo4j, which is a graph database and visualizing software.
00:55:21	Alyssa Ibarra:	Nope. You pretty much covered it.
00:56:28	Ryan Woodbury:	Great job, Carlo. Got to run.
00:56:43	Alyssa Ibarra:	It was great! Thank you so much.
```
</details>



# Walkthrough 7: The Role (and Usefulness) of Multilevel Models

**Learning objectives:**

+
+
+

## Slides

`r knitr::include_url("./R/2021-04-21/walkthrough-7.html")`

## Meeting Videos

### Cohort 1

`r knitr::include_url("https://www.youtube.com/embed/mkw55WuyP9w")`


<details>
  <summary> Meeting chat log </summary>
  
```
00:04:12	Mike Haugen:	Is there a particular way to share my screen. I have the slides in a browser?
00:37:08	Ryan Woodbury:	Stroop task
00:45:05	Isabella Vel√°squez:	are you able to run multilevel models in tidymodels?
00:47:10	Edgar Zamora:	https://github.com/tidymodels/multilevelmod
00:50:00	Alyssa Ibarra:	Sorry, laptop battery died.
00:51:46	Alyssa Ibarra:	Thanks!
00:51:59	Carlo Medina:	thank you!
```
</details>



# Walkthrough 8: Predicting Students‚Äô Final Grades Using Machine Learning Methods with Online Course Data

**Learning objectives:**

+
+
+

## Slides

`r knitr::include_url("./R/2021-05-05/walkthrough-8.html")`

## Meeting Videos

### Cohort 1

`r knitr::include_url("https://www.youtube.com/embed/5fDoXCyDego")`


<details>
  <summary> Meeting chat log </summary>
  
```
00:22:38	Rob Lucas (he/him):	Does anyone have any preferred resources on feature engineering?
00:24:03	Carlo Medina:	I have not done modelling for quite a while. would love to hear resources too, if any.
00:33:05	Mike Haugen:	Max Kuhn
00:33:22	shamsuddeen:	http://www.feat.engineering
00:33:57	Mike Haugen:	I am reading that book now, it is great because it goes beyond the software/code and explains the modeling process, stats a bit.
00:39:13	Mike Haugen:	Max has a book on the caret package as well: http://topepo.github.io/caret/index.html
00:39:36	Mike Haugen:	And tidy models: https://www.tmwr.org/
00:40:20	Rob Lucas (he/him):	Wow, he is prolific. Thanks!
00:43:28	Mike Haugen:	Julia Sigle‚Äôs video blog has a great example of using random forest with tidy models: https://juliasilge.com/blog/ikea-prices/
00:43:31	shamsuddeen:	http://www.feat.engineering
00:43:36	shamsuddeen:	http://appliedpredictivemodeling.com
00:48:52	Mike Haugen:	Love variable importance; easy for others to understand.
00:54:27	Carlo Medina:	thanks Shamsuddeen!
00:54:30	Mike Haugen:	Thanks!
```
</details>



<!-- #  : -->

<!-- **Learning objectives:** -->

<!-- + -->
<!-- + -->
<!-- + -->

<!-- ## Slides -->

<!-- `r knitr::include_url("./R/2021-0-/.html")` -->

<!-- ## Meeting Videos -->

<!-- ### Cohort 1 -->

<!-- `r knitr::include_url("https://www.youtube.com/embed/")` -->


<!-- <details> -->
<!--   <summary> Meeting chat log </summary> -->

<!-- ``` -->

<!-- ``` -->
<!-- </details> -->

