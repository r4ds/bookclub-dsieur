[["index.html", "Data Science in Education Using R Book Club Welcome", " Data Science in Education Using R Book Club The R4DS Online Learning Community 2021-04-21 Welcome This is a companion for the book Data Science in Education Using R by Ryan A. Estrellado, Emily A. Bovee, Jesse Mostipak, Joshua M. Rosenberg, and Isabella C. VelaÃÅsquez. This companion is available at r4ds.io/dsieur. This website is being developed by the R4DS Online Learning Community. Follow along, and join the community to participate. This companion follows the R4DS Online Learning Community Code of Conduct. "],["book-club-meetings.html", "Book club meetings", " Book club meetings Each week, a volunteer will present a chapter from the book. This is the best way to learn the material. Presentations will usually consist of a review of the material, a discussion, and/or a demonstration of the principles presented in that chapter. More information about how to present is available in the github repo. Presentations will be recorded, and will be available on the R4DS Online Learning Community YouTube Channel. "],["introduction-and-how-to-use-this-book.html", "Chapter 1 Introduction and How to Use this Book", " Chapter 1 Introduction and How to Use this Book Learning objectives: Intros &amp; get to know each other Understanding community expectations Reviewing the reading schedule Information on the weekly meetings Information on using GitHub "],["slides.html", "1.1 Slides", " 1.1 Slides "],["meeting-videos.html", "1.2 Meeting Videos", " 1.2 Meeting Videos 1.2.1 Cohort 1 Meeting chat log 00:03:53 Morgan Grovenburg: I saw the book mentioned a DataEdu Slack Channel (https://dataedu.slack.com/). Is that still going/active? 00:05:10 Jon Harmon (jonthegeek): Yes it is! It&#39;s not as active as R4DS, but there are usually discussions happening. 00:05:10 Isabella Vel√°squez: Hey Morgan! That‚Äôs is actually where the book got started :) There are lots of people on that Slack. I can send an invite link to join if anybody is interested. 00:05:36 Morgan Grovenburg: Yes I&#39;m interested in an invite! 00:05:40 Rob Lucas: Me too! 00:06:12 Isabella Vel√°squez: Here it is! Let‚Äôs work together in Slack! It‚Äôs a faster, simpler way to talk shop, share files, and get work done. Join here: https://join.slack.com/t/dataedu/shared_invite/zt-9jkexbww-QxFbexhFoanFicVWaEpyBA 00:16:55 beth.kelly: I prefer the camera too! Sorry. 00:18:23 Isabella Vel√°squez: Thanks Beth! Same here, I often feel siloed and I can‚Äôt wait to learn from all your work. 00:21:10 Jon Harmon (jonthegeek): r4ds.io/diseur will take you to the bookdown side of the repo, and allow you to hop over to the GitHub (assuming I set it up correctly) 00:22:04 Layla Bouzoubaa: John I get a 404 00:22:15 Alyssa Ibarra: Same here 00:22:47 Ryan Woodbury: https://github.com/r4ds/bookclub-dsieur 00:23:19 Jon Harmon (jonthegeek): Hmmm. Evidently it requires https://r4ds.io/dsieur 00:23:35 Jon Harmon (jonthegeek): ...which might have just solved a bug I&#39;ve had elsewhere. 00:24:04 Layla Bouzoubaa: sweeet 00:28:08 beth.kelly: Jon is freezing for everyone else, right? 00:28:12 Morgan Grovenburg: Yes 00:28:12 Alyssa Ibarra: I think you‚Äôre freezing, Jon 00:28:17 Daniel Jin: Y 00:28:51 Mark LaVenia: https://github.com/r4ds/bookclub-dsieur 00:29:52 Daniel Jin: 50% this time tho! 00:30:16 Jon Harmon (jonthegeek): I will talk about this on the Slack evidently üôÉ 00:30:28 Isabella Vel√°squez: yeah absolutely. Can‚Äôt wait to get any feedback!! 00:33:12 Jon Harmon (jonthegeek): My internet is really bad today, gonna drop off! 00:33:28 Ryan Woodbury: Thanks for setting this up! See you soon. 00:37:28 Isabella Vel√°squez: Yes! And we‚Äôll send a Slack message once we‚Äôre done. 00:39:46 Ryan Woodbury: I listened to a podcast about the Kentucky stats project. It sounded way cool! 00:41:44 beth.kelly: That&#39;s great! I&#39;d love to hear about the podcast. 00:42:19 beth.kelly: I love the desks in your background Rob! 00:43:14 Rob Lucas: Thanks, Beth! They came out of a school in Ohio where my grandfather was a principal. 00:43:28 Isabella Vel√°squez: So glad to hear it!! 00:46:12 Michael H: Ryan E‚Äôs podcast About Practice is great 00:46:39 Ryan Woodbury: ^^ Yes ^^ 00:49:38 beth.kelly: So interested in retention in higher ed! 00:51:28 Ryan Woodbury: Here is the podcast link the ‚ÄúData Ed Chat‚Äù, episode 10 discusses the Kentucky Stats Explorer: https://www.buzzsprout.com/1074286 00:51:45 Ryan Woodbury: And the ‚ÄúAbout Practice‚Äù podcast with two of the books authors: https://anchor.fm/about-practice/episodes/1-Hypothesis-Testing-for-Kids-is-Basically-Torture-eq0qc6 00:53:48 Morgan Grovenburg: I think that was everyone 00:54:30 Mark LaVenia: Thank you! :) 00:54:55 Daniel Jin: Thank you Ryan and Isabella for organizing! Can&#39;t wait! 00:55:00 Mark LaVenia: I&#39;ll stick around for that too :) 00:55:13 Marina: This is my first time participating in something like this. Usually, how long do these sessions last? 00:56:01 Ryan Woodbury: https://www.youtube.com/watch?v=d41oc2OMAuI&amp;feature=youtu.be 00:56:49 Ryan Woodbury: The sessions are blocked for an hour. However, I have seen some videos from other book clubs that last for 30 minutes. It‚Äôll depend on the week. 00:57:31 Marina: Ok, thanks! 01:08:02 Mark LaVenia: very helpful. thanks! "],["what-does-data-science-in-education-look-like.html", "Chapter 2 What does Data Science in Education Look Like?", " Chapter 2 What does Data Science in Education Look Like? Learning objectives: TBD "],["slides-1.html", "2.1 Slides", " 2.1 Slides "],["meeting-videos-1.html", "2.2 Meeting Videos", " 2.2 Meeting Videos 2.2.1 Cohort 1 TBD Meeting chat log 00:12:11 Rob Lucas: PollEv.com/roblucas198 00:14:20 Daniel Jin: sorry have to step away for a moment! 00:21:24 Marina: I don&#39;t currently work in education so I&#39;m going to skip this question (: 00:32:17 Mark LaVenia: data curation is a big part of my role 00:32:41 Ronak Patel: I have to hop off for a work call but will make sure to carve out an hour here on out. Thanks all! 00:32:47 Alyssa Ibarra: what is data curation? Sorry not familiar with the term 00:34:00 Mark LaVenia: @Alyssa - Finding data that teams need to answer a question, etc. 00:34:16 Alyssa Ibarra: Ah yes, thanks! 00:39:58 Mark LaVenia: data interrogation 00:42:15 Mark LaVenia: need to, but don&#39;t 00:42:20 Ryan Woodbury: same 00:42:27 Marina: ^^^ 00:56:18 Marina: I know this is an R-focused thing, but for basic/quick spatial analysis, I&#39;ve really enjoyed using QGIS, which is more point and click 00:57:41 Mark LaVenia: thanks @Marina. Really trying to stretch into R and Python. Thanks for the tip on QGIS; will check that out 01:02:57 Mark LaVenia: +1 01:03:52 Mark LaVenia: Nicely done Rob "],["special-considerations.html", "Chapter 3 Special Considerations", " Chapter 3 Special Considerations Learning objectives: Understand data science and change management Discuss reproducibility in our work Share resources "],["slides-2.html", "3.1 Slides", " 3.1 Slides 3.1.1 Resources Shared Switch: How to Change Things When Change is Hard by Chip Heath, Dan Heath ADKAR Model Education Data Done Right Weapons of Math Destruction by Cathy O‚ÄôNeil Thinking in Systems by Donella H. Meadows We All Count Urban Institute Applying Racial Equity Awareness in Data Visualization Education Equity Indicator Systems "],["meeting-videos-2.html", "3.2 Meeting Videos", " 3.2 Meeting Videos 3.2.1 Cohort 1 Meeting chat log 00:10:51 Isabella Vel√°squez: Here&#39;s the Github for the bookclub! https://github.com/r4ds/bookclub-dsieur 00:11:02 Catherine Miller: thank you! 00:25:22 Catherine Miller: 5 levels, maybe? instructors, aides, administrators, executives, data folks 00:26:17 Rob Lucas (he/him): 5 seems about right, depending where you divide them up 00:27:02 Yukie Toyama: Yeah 5 groups: researchers, funding organization, teachers, students, and parents/public 00:27:04 Isabella Vel√°squez: We cut across different strategies on our team. Top ones are leadership, non-technical team members, technical team members, external partners 00:27:37 Catherine Miller: the technical and non-technical categories are useful for this 00:28:00 Catherine Miller: plus external partners who see a different grain size of data 00:28:19 Mark LaVenia: We have an Executive Director, Chiefs, Directors, Specialists, and Managers--and my work requires working across and between all levels 00:28:47 Isabella Vel√°squez: yes. data privacy concerns are really important when considering what data is being shared 00:28:53 Catherine Miller: Thinking in Systems is also useful? Donella Meadows 00:32:39 Catherine Miller: that is cool 00:42:23 Catherine Miller: https://www.eddatadoneright.com/blog/ 00:45:02 Isabella Vel√°squez: A couple of resources I&#39;m digging into: 00:45:17 Isabella Vel√°squez: Urban Institute Applying Racial Equity Awareness in Data Visualization https://urban-institute.medium.com/applying-racial-equity-awareness-in-data-visualization-bd359bf7a7ff 00:45:45 Isabella Vel√°squez: National Academies of Sciences, Engineering, and Medicine https://www.nap.edu/catalog/25833/building-educational-equity-indicator-systems-a-guidebook-for-states-and 00:47:05 Edgar Zamora: https://weallcount.com 00:50:20 Isabella Vel√°squez: great points!! 00:55:44 Mark LaVenia: Thank you! "],["getting-started-with-r-and-rstudio-foundational-skills.html", "Chapter 4 Getting Started with R and RStudio &amp; Foundational Skills", " Chapter 4 Getting Started with R and RStudio &amp; Foundational Skills Learning objectives: Knitting ppt slides from Rmd Writing function Reading/nterpretting code Avoiding/debugging typos Piping %&gt;% and convention preference &lt;- vs -&gt; "],["slides-3.html", "4.1 Slides", " 4.1 Slides Foundational Skills Script `# using the filter() function from the stats package x &lt;- 1:100 stats::filter(x, rep(1, 3)) `# using the filter() function from the dplyr package starwars %&gt;% dplyr::filter(mass &gt; 85) dataedu::ma_data_init dataedu::ma_data_init -&gt; ma_data ma_data_init &lt;- dataedu::ma_data_init # you probably wrote these 3 library() lines in your R script file earlier # if you have not yet run them, you will need to run these three lines before running the rest of the chunk library(tidyverse) library(dataedu) library(skimr) library(janitor) # Exploring and manipulating your data names(ma_data_init) glimpse(ma_dat_init) glimpse(ma_data_init) summary(ma_data_init) glimpse(ma_data_init$Town) summary(ma_data_init$Town) glimpse(ma_data_init$AP_Test Takers) glimpse(ma_data_init$`AP_Test Takers`) summary(ma_data_init$`AP_Test Takers`) ma_data_init %&gt;% group_by(District Name) %&gt;% count() ma_data_init %&gt;% group_by(`District Name`) %&gt;% count() ma_data_init %&gt;% group_by(`District Name`) %&gt;% count() %&gt;% filter(n &gt; 10) ma_data_init %&gt;% group_by(`District Name`) %&gt;% count() %&gt;% filter(n &gt; 10) %&gt;% arrange() "],["meeting-videos-3.html", "4.2 Meeting Videos", " 4.2 Meeting Videos 4.2.1 Cohort 1 Meeting chat log 00:06:21 Layla Bouzoubaa: Hello everyone 00:06:47 Catherine Miller: Hello! 00:07:29 Mark LaVenia: Hi everyone 00:13:40 Ronak Patel: 3 minutes in and I&#39;ve already learned something new! 00:13:51 Daniel Jin: +1 00:13:51 Morgan Grovenburg: Me too! 00:14:53 Ryan Woodbury: It looks way better in the book down compared to Google Slides 00:14:56 Carlo M: reproducibility 00:16:17 Catherine Miller: thanks, that was helpful 00:17:25 Mike Haugen: The R markdown chapter in R4DS goes over benefits: https://r4ds.had.co.nz/r-markdown.html 00:22:30 shamsuddeen: New approach for installing packages: https://github.com/r-lib/pak 00:26:20 Ryan Woodbury: YES!!! I just used it! 00:26:36 Carlo M: Syntactic sugar of ifelse(is.na(certification), certification_1, certification) 00:27:04 Marina: ^^ yeah, I had been using ifelse, but this looks neater 00:27:05 Ryan Woodbury: Trying to combine cohort data where each cohort has different variable names. 00:30:42 Alyssa Ibarra: According to Help, coalesce is from dplyr 00:30:55 Layla Bouzoubaa: janitor 00:30:59 shamsuddeen: coalesce is from dplyr? 00:31:09 Layla Bouzoubaa: janitor::remove_empty() 00:31:14 shamsuddeen: Oh yah, I see it 00:31:25 Carlo M: Yup, coalesce is dplyr 00:37:22 Layla Bouzoubaa: If you do in the ?stats::filter console, it is for time series data 00:37:32 Layla Bouzoubaa: He‚Äôs correct 00:37:43 Alyssa Ibarra: Thanks for that clarification! 00:39:55 Morgan Grovenburg: Always &lt;- 00:40:00 Mike Haugen: ^^ 00:40:14 Ronak Patel: -&gt; gives me anxiety 00:40:28 Carlo M: When I get lazy, I end up using `-&gt;` 00:40:43 Morgan Grovenburg: My reason is to keep = straight 00:40:48 Isabella Vel√°squez: same! I only use -&gt; when I don‚Äôt want to scroll all the way to the beginning of a pipe XD 00:40:54 Layla Bouzoubaa: Second Ronan‚Äôs point 00:41:01 Layla Bouzoubaa: ronak* 00:42:53 Isabella Vel√°squez: I once spent a lot of time before realizing I used ‚Äò instead of ` 00:46:40 Carlo M: Morgan is right. The {stats} library would get masked when library(dplyr) is called 00:50:07 shamsuddeen: Happy Git with R is a good book on learning Gihub: https://happygitwithr.com 00:52:56 Layla Bouzoubaa: Sorry everyone I need to hop off. Cheers! 00:53:01 Mike Haugen: How do you get a branch? 00:54:53 Alyssa Ibarra: ^ same question 00:56:29 Catherine Miller: Thank you Mark :) 00:56:31 Marina: This discussion is giving me scary git flashbacks 00:56:39 Catherine Miller: githell, right? 00:57:20 Marina: ^^^ 00:58:53 Carlo M: You‚Äôre doing great!! 00:59:16 Catherine Miller: I always get lost here, too. 01:02:06 Isabella Vel√°squez: I always create a branch by clicking the button next to main in R Studio 01:03:48 Ronak Patel: I gotta hop off for another meeting. Thanks, Mark. Great job today! See ya&#39;ll next week. 01:03:49 Catherine Miller: Thank you!! 01:04:18 Catherine Miller: Thank you for being brave and helping us learn, Mark!! 01:04:20 shamsuddeen: Thanks 01:04:24 Alyssa Ibarra: Thank you! 01:04:37 Carlo M: Thanks Mark! "],["walkthrough-1-the-education-data-science-pipeline.html", "Chapter 5 Walkthrough 1: The Education Data Science Pipeline", " Chapter 5 Walkthrough 1: The Education Data Science Pipeline Learning objectives: Learning about key steps in data preparation, processing, wrangling using tidyverse (a set of packages). "],["slides-4.html", "5.1 Slides", " 5.1 Slides Walkthrough 1 Script # useful shortcut# # Run current line/selection: cmd + return (MAC), ctrl + Enter (Windows) # Assignment sign (&lt;-) : Option + - (M), Alt + - (W) # Pipe sign (%) : cmd + shift + M (M), ctrl + shift + M (W) install.packages(c(&quot;tidyverse&quot;, &quot;apaTables&quot;, &quot;sjPlot&quot;, &quot;dataedu&quot;, &quot;summarytools&quot;, &quot;ggpubr&quot;)) # dataedu wasn&#39;t available for R ver 3.6.3 so I installed dev version of dataedu remotes::install_github(&quot;data-edu/dataedu&quot;) # this was not in the book but useful to get descriptives install.packages(&quot;summarytools&quot;) # Load packages library(tidyverse) library(apaTables) library(sjPlot) library(readxl) library(dataedu) library(summarytools) library(ggpubr) ############################ # import data from dataedu # ############################ # Pre-survey for the F15 and S16 semesters pre_survey &lt;- dataedu::pre_survey # Gradebook and log-trace data for F15 and S16 semesters course_data &lt;- dataedu::course_data # Log-trace data for F15 and S16 semesters - this is for time spent course_minutes &lt;- dataedu::course_minutes ############# # view data # ############# pre_survey head(pre_survey) # first six rows View(pre_survey) # a full view in a separate tab glimpse(pre_survey) # list of variables, values for the first couple of cases # get a quick look at each variable in df view(dfSummary(pre_survey)) ## take a look at course_data, course_minutes; what do you notice? ######################## # 1.process pre_survey # ######################## pre_survey &lt;- pre_survey %&gt;% # Rename the qustions something easier to work with because R is case sensitive # and working with variable names in mix case is prone to error rename( q1 = Q1MaincellgroupRow1, q2 = Q1MaincellgroupRow2, q3 = Q1MaincellgroupRow3, q4 = Q1MaincellgroupRow4, q5 = Q1MaincellgroupRow5, q6 = Q1MaincellgroupRow6, q7 = Q1MaincellgroupRow7, q8 = Q1MaincellgroupRow8, q9 = Q1MaincellgroupRow9, q10 = Q1MaincellgroupRow10 ) %&gt;% # Convert all question responses to numeric mutate_at(vars(q1:q10), list( ~ as.numeric(.))) # q1-10 are already numeric, so this doesn&#39;t seem necessary # you could insert suffix to the var names indicate the three dimensions # e.g. q1.i, q2.uv, 3.pc where i = interest, u = utility value, p = perceived competence ############################################# #1a.practice mutate = making a new variable # ############################################# # create a df (dataframe) in tibble format with two columns/vars: male &amp; female df &lt;- tibble( male = 5, female = 5 ) # Use mutate to create a new column called &quot;total_students&quot; # populate that column with the sum of the &quot;male&quot; and &quot;female&quot; variables df %&gt;% mutate(total_students = male + female) # let&#39;s keep this new column in df df &lt;- df %&gt;% mutate(total_students = male + female) ###################################################### # 1b. reverse_score function with mutate &amp; case_when # ###################################################### # This part of the code is where we write the function: # Function for reversing scales reverse_scale &lt;- function(question) { # Reverses the response scales for consistency # Arguments: # question - survey question # Returns: # a numeric converted response # Note: even though 3 is not transformed, case_when expects a match for all # possible conditions, so it&#39;s best practice to label each possible input # and use TRUE ~ as the final statement returning NA for unexpected inputs x &lt;- case_when( question == 1 ~ 5, question == 2 ~ 4, question == 3 ~ 3, question == 4 ~ 2, question == 5 ~ 1, TRUE ~ NA_real_ ) x } # let&#39;s see how it works reverse_scale(pre_survey$q4) pre_survey$q4 # compare with original # And here&#39;s where we use that function to reverse the scales # We use the pipe operator %&gt;% here # Reverse scale for questions 4 and 7 pre_survey &lt;- pre_survey %&gt;% mutate(q4 = reverse_scale(q4), # mutate with the original var name to overwrite q7 = reverse_scale(q7)) # Note: psych package has reverse.code() function so you don&#39;t have to write your own ##################################################### #1c. pivot_longer to make pre_survey into long form # ##################################################### # Pivot the dataset from wide to long format # And name the long format df as measure_mean measure_mean &lt;- pre_survey %&gt;% # Gather questions and responses pivot_longer(cols = q1:q10, names_to = &quot;question&quot;, # give a new var/col name &quot;question&quot; where question # will go values_to = &quot;response&quot;) # give a new var/col name &quot;response&quot; where response values will go # create a new var called measure to denote 3 dimensions of motivation measure_mean &lt;- measure_mean %&gt;% # Here&#39;s where we make the column of question categories called &quot;measure&quot; mutate( measure = case_when( question %in% c(&quot;q1&quot;, &quot;q4&quot;, &quot;q5&quot;, &quot;q8&quot;, &quot;q10&quot;) ~ &quot;int&quot;, question %in% c(&quot;q2&quot;, &quot;q6&quot;, &quot;q9&quot;) ~ &quot;uv&quot;, question %in% c(&quot;q3&quot;, &quot;q7&quot;) ~ &quot;pc&quot;, TRUE ~ NA_character_) ) ################################################### # 1d. Get mean scores for each motivation measure # # Across ~912 students who responded the pre-survey # using group_by() and summarize() ################################################### measure_mean &lt;- measure_mean %&gt;% # First, we group by the new variable &quot;measure&quot; group_by(measure) %&gt;% # Here&#39;s where we compute the mean of the responses summarize( # Creating a new variable to indicate the mean response for each measure mean_response = mean(response, na.rm = TRUE), # Creating a new variable to indicate the percent of each measure that # had NAs in the response field percent_NA = mean(is.na(response)) ) measure_mean ############################## # 2. Process the course data # ############################## View(course_data) # split course section into components course_data &lt;- course_data %&gt;% # Give course subject, semester, and section their own columns separate( col = CourseSectionOrigID, into = c(&quot;subject&quot;, &quot;semester&quot;, &quot;section&quot;), sep = &quot;-&quot;, remove = FALSE # this is to keep the original var ) ############################################# # 3. Join/merge course_data with pre_survey # ############################################# #rename pre_survey id vars pre_survey &lt;- pre_survey %&gt;% rename(student_id = opdata_username, #new_var_name = old_var_name course_id = opdata_CourseID) pre_survey ################################################ #3a. extract 5 digits inbetween _ _ in student_id # ################################################ #trying str_sub just with one string value str_sub(&quot;_99888_1&quot;, start = 2) str_sub(&quot;_99888_1&quot;, start = -3) str_sub(&quot;_99888_1&quot;, start = 2, end = -3) # Re-create the variable &quot;student_id&quot; so that it excludes the extraneous characters pre_survey &lt;- pre_survey %&gt;% mutate(student_id = str_sub(student_id, start = 2, end = -3)) # Save the new variable as numeric so that R no longer thinks it is text pre_survey &lt;- pre_survey %&gt;% mutate(student_id = as.numeric(student_id)) ########################################################### #3b rename id vars in course_data and join with pre_survey ########################################################## course_data &lt;- course_data %&gt;% rename(student_id = Bb_UserPK, course_id = CourseSectionOrigID) # new df merges course_data with pre_survey dat &lt;- left_join(course_data, pre_survey, by = c(&quot;student_id&quot;, &quot;course_id&quot;)) dat ############################################ #4. Process course_minutes &amp; join with dat ############################################ course_minutes &lt;- course_minutes %&gt;% rename(student_id = Bb_UserPK, course_id = CourseSectionOrigID) course_minutes &lt;- course_minutes %&gt;% # Change the data type for student_id in course_minutes so we can match to # student_id in dat mutate(student_id = as.integer(student_id)) dat &lt;- dat %&gt;% left_join(course_minutes, by = c(&quot;student_id&quot;, &quot;course_id&quot;)) # dat has many gradebook_items per student per course # we want just one row per student &amp; course combo # using distinct() dat &lt;- distinct(dat, course_id, student_id, .keep_all = TRUE) # rename final grade var dat &lt;- rename(dat, final_grade = FinalGradeCEMS) ########################################### # 5. Analysis ########################################### ####################################################################### # 5a.Scatter plot to examine relationship between final grade &amp; time spent ####################################################################### view(dfSummary(dat)) #scatter plot to see relationship between timespent &amp; final grade p1 &lt;- dat %&gt;% # aes() tells ggplot2 what variables to map to what feature of a plot # Here we map variables to the x- and y-axis ggplot(aes(x = TimeSpent, y = final_grade)) + # Creates a point with x- and y-axis coordinates specified above geom_point(color = dataedu_colors(&quot;green&quot;)) + theme_dataedu() + labs(x = &quot;Time Spent&quot;, y = &quot;Final Grade&quot;) # add a line of best fit p1 + geom_smooth(method = &quot;lm&quot;) # with ggpubr, you can add correlation to the graph require(ggpubr) p2 &lt;- ggscatter(dat, x = &quot;TimeSpent&quot;, y = &quot;final_grade&quot;, color = &quot;springgreen4&quot;, add = &quot;reg.line&quot;, # Add regressin line add.params = list(color = &quot;blue&quot;, fill = &quot;lightgray&quot;), # Customize reg. line conf.int = TRUE # Add confidence interval ) # Add correlation coefficient p2 + stat_cor(method = &quot;pearson&quot;, label.x = 3900, label.y = 130, p.accuracy = 0.001, r.accuracy = 0.01) ################################################### # 5b.Linear regression with time spent as predictor ################################################### m_linear &lt;- lm(final_grade ~ TimeSpent, data = dat) summary(m_linear) # get publication ready table with tab_model function require(sjPlot) tab_model(m_linear, title = &quot;Table 7.1&quot;) # you can copy and paste it into Word! # or save it with apa.re.table function apa.reg.table(m_linear, filename = &quot;regression-table-output.doc&quot;) ################################################### # 5c.Correlations among the 3 motivation variables ################################################### # pivot survey_responses to long form survey_responses &lt;- pre_survey %&gt;% # Gather questions and responses pivot_longer(cols = q1:q10, names_to = &quot;question&quot;, values_to = &quot;response&quot;) %&gt;% mutate( # Here&#39;s where we make the column of question categories measure = case_when( question %in% c(&quot;q1&quot;, &quot;q4&quot;, &quot;q5&quot;, &quot;q8&quot;, &quot;q10&quot;) ~ &quot;int&quot;, question %in% c(&quot;q2&quot;, &quot;q6&quot;, &quot;q9&quot;) ~ &quot;uv&quot;, question %in% c(&quot;q3&quot;, &quot;q7&quot;) ~ &quot;pc&quot;, TRUE ~ NA_character_ )) # create mean_response for each student for each measure survey_responses &lt;- survey_responses %&gt;% group_by(student_id, measure) %&gt;% # Here&#39;s where we compute the mean of the responses for each stdt &amp; measure combo summarize( # Mean response for each measure mean_response = mean(response, na.rm = TRUE) ) # Filter NA (missing) responses and pivot to wide form survey_responses &lt;- survey_responses %&gt;% filter(!is.na(mean_response)) %&gt;% pivot_wider(names_from = measure, values_from = mean_response) survey_responses # get correlation table survey_responses %&gt;% apa.cor.table(filename = &quot;corr-table-output.doc&quot;) # note the correlation table includes student_id. # probably want to delete it in Word ############################################################################# # 5d. Linear regression with hours sptent (rather than minutes) as predictor ############################################################################ # creating a new variable for the amount of time spent in hours dat &lt;- dat %&gt;% mutate(TimeSpent_hours = TimeSpent / 60) # the same linear model as above, but with the TimeSpent variable in hours m_linear_1 &lt;- lm(final_grade ~ TimeSpent_hours, data = dat) # viewing the output of the linear model tab_model(m_linear_1, title = &quot;Table 7.2&quot;) ################################################################## # 5e. Linear regression with standardized time spent as predictor ################################################################## # this is to standardize the TimeSpent variable to have a mean of 0 and a standard deviation of 1 # this makes intercept more interpretable dat &lt;- dat %&gt;% mutate(TimeSpent_std = scale(TimeSpent)) # the same linear model as above, but with the TimeSpent variable standardized m_linear_2 &lt;- lm(final_grade ~ TimeSpent_std, data = dat) # viewing the output of the linear model tab_model(m_linear_2, title = &quot;Table 7.3&quot;) ##################################################################### #6. Multiple regression model with time spent &amp; subject as predictors ##################################################################### # a linear model with the subject added # independent variables, such as TimeSpent_std and subject, can simply be separated with a plus symbol: m_linear_3 &lt;- lm(final_grade ~ TimeSpent_std + subject, data = dat) # note: subject is a categorical variable and it seems AnPhA (animal physiology) # is set as the reference category (numbers assinged by alphabetical order) tab_model(m_linear_3, title = &quot;Table 7.4&quot;) # Combine all four models in one table &amp; show standard errors rather than CIs tab_model(m_linear, m_linear_1, m_linear_2, m_linear_3, show.ci = FALSE, show.se = TRUE) ##################################################################### #7. What other analyses can you think of? ##################################################################### # Add total scores of pre-course motivation as a predictor? # --&gt; use mutate to create sum_motiv variable # Does the effect of time spent vary by subjects/courses? # --&gt; add time x subject interaction term # Maybe color dots in scatter plot by subject to see if there is a pattern ggplot(data = dat, aes(x = TimeSpent, y = final_grade, color = subject)) + geom_point() + theme_dataedu() + labs(x = &quot;Time Spent&quot;, y = &quot;Final Grade&quot;) "],["meeting-videos-4.html", "5.2 Meeting Videos", " 5.2 Meeting Videos 5.2.1 Cohort 1 Meeting chat log 00:04:35 Ryan Woodbury: https://rfordatascience.slack.com/files/UQ4DR12BY/F01QUFD8V5H/dsieur_ch7_slides 00:04:50 Ryan Woodbury: https://rfordatascience.slack.com/files/UQ4DR12BY/F01RJ4ENF4Y/desieur_ch7_scripts.r 00:04:55 Ryan Woodbury: Slides, then script 00:10:33 Ryan Woodbury: Is it like skimr? 00:14:57 Isabella Vel√°squez: super clear! love the color coding 00:16:26 Edgar Zamora: https://www.garrickadenbuie.com/project/tidyexplain/ I use these GIFs to help me visualize the different kind of joins. Get confused 00:17:31 Rob Lucas: Thanks for sharing that Edgar! The semi and anti joins were new to me. I think this will help me visualize them. 00:26:26 Mark LaVenia: you are doing great! Thanks! 00:29:29 Ryan Woodbury: The `mutate_*()` functions are being superseded by using the `across()` function within `mutate()`. 00:30:03 Isabella Vel√°squez: here&#39;s some documentation on the mutate_* functions. https://dplyr.tidyverse.org/reference/mutate_all.html but like Ryan said, they&#39;ve been superseded. I am still learning across()! 00:30:05 Ryan Woodbury: Line 69 would be: mutate(across(q1:q10, as.numeric)) in the &quot;new&quot; format 00:30:37 Alyssa Ibarra: Thanks, Ryan! I didn&#39;t know it was changing 00:30:49 Ryan Woodbury: I was just getting used to the mutate_*() functions too! 00:37:57 Alyssa Ibarra: when would you use mutate versus transmute? 00:38:35 Ryan Woodbury: The issue with the psych::reverse.code() function that Yukie is talking about is that it is not tidyverse friendly, *but* is a function that is already made and does a great job. (I love the psych package, BTW.) 00:45:41 Mark LaVenia: I love that 01:00:27 Ryan Woodbury: Great advice on imagining the joined datasets. 01:01:19 Isabella Vel√°squez: I&#39;ve got to hop off at 5. Thank you SO much Yukie! That was fantastic ! 01:01:39 Ryan Woodbury: Thank you! Great work. 01:02:58 Mark LaVenia: Great job Yukie! 01:03:00 Alyssa Ibarra: Thank you so much! It was so great! 01:03:09 Edgar Zamora: Great job! Thank you! "],["walkthrough-2-approaching-gradebook-data-from-a-data-science-perspective.html", "Chapter 6 Walkthrough 2: Approaching Gradebook Data From a Data Science Perspective", " Chapter 6 Walkthrough 2: Approaching Gradebook Data From a Data Science Perspective Learning objectives: Learn about importing excel spreadsheets using a file path or here(), tidying data with clean_names() and remove_empty() from the {janitor} package, transforming data with pivot_longer() using contains() from the {stringr} package, visualizing data, correlation analysis with cor(), and modeling data with lm(). "],["slides-5.html", "6.1 Slides", " 6.1 Slides Walkthrough 2 Rmd Script "],["meeting-videos-5.html", "6.2 Meeting Videos", " 6.2 Meeting Videos 6.2.1 Cohort 1 Meeting chat log 00:24:07 Rob Lucas: Do you all use {janitor}? The book mentioned that it was developed by an education person, and I wondered if it was used more in education contexts. 00:24:23 Ronak Patel: clean_names() has changed my life 00:24:26 Alyssa Ibarra: I have never used it before this. 00:24:39 edgarzamora: ^^^ Ronak +1 00:24:45 Isabella Vel√°squez: +1 to clean_names()! 00:25:55 Isabella Vel√°squez: {janitor} is by Sam Firke from The New Teacher Project, but I think can be used in any data cleaning context : https://github.com/sfirke/janitor 00:26:34 Mike Haugen: And then when you create objects, do you make them snake_case? 00:26:38 Mike Haugen: https://r4ds.had.co.nz/workflow-basics.html?q=snake_case#whats-in-a-name 00:27:32 Isabella Vel√°squez: usually I do. I only use camelCase in Shiny 00:27:50 edgarzamora: By default it is snake_case 00:29:56 edgarzamora: select(-c(absent, late)) 00:30:30 Ryan Woodbury: I do it that way too 00:30:54 Ronak Patel: same, especially when exploring data. never know what you want to drop and it saves some time. 00:31:18 Isabella Vel√°squez: are these xaringan slides? so cool 00:37:38 Rob Lucas: Why color code those if the boxplots aren&#39;t even ordered correctly? 00:38:06 Ronak Patel: does anyone have a quick way to order them? 00:38:34 edgarzamora: fct_reorder() maybe? 00:38:58 Ryan Woodbury: Check out, reorder_within() from tidytext 00:40:03 Ryan Woodbury: https://juliasilge.com/blog/reorder-within/ 00:50:54 Isabella Vel√°squez: Awesome job!!! 00:53:38 Ronak Patel: in a regression, isn&#39;t independence of variables an assumption? 00:54:03 Ryan Woodbury: The errors are independent 00:54:06 Ryan Woodbury: Should be 00:55:10 Ryan Woodbury: That is a good question. I want to help find an answer! 00:55:46 Mike Haugen: Thanks! "],["walkthrough-3-using-school-level-aggregate-data-to-illuminate-educational-inequities.html", "Chapter 7 Walkthrough 3: Using School-Level Aggregate Data to Illuminate Educational Inequities", " Chapter 7 Walkthrough 3: Using School-Level Aggregate Data to Illuminate Educational Inequities Learning objectives: This chapter explores what aggregate data is, and how to access, clean, and explore it. "],["slides-6.html", "7.1 Slides", " 7.1 Slides "],["meeting-videos-6.html", "7.2 Meeting Videos", " 7.2 Meeting Videos 7.2.1 Cohort 1 Meeting chat log 00:26:06 Ryan Woodbury: Here&#39;s a bit of history and potential future directions for FRL metric: https://dataqualitycampaign.org/resource/accurate-student-poverty-data-is-crucial-to-supporting-all-students/ 00:30:14 Isabella Vel√°squez: here&#39;s another reference on FRPL; Urban Institute is coming up with an alternative measure (disclosure: my coworker is funding this project): https://www.urban.org/features/measuring-student-poverty-dishing-alternatives-free-and-reduced-price-lunch 00:31:39 Isabella Vel√°squez: also want to share my favorite tweet :) https://twitter.com/andrewheiss/status/1021944992351186944?s=21 00:36:34 Ronak Patel: Thanks for sharing those articles! "],["walkthrough-4-longitudinal-analysis-with-federal-students-with-disabilities-data.html", "Chapter 8 Walkthrough 4: Longitudinal Analysis With Federal Students With Disabilities Data", " Chapter 8 Walkthrough 4: Longitudinal Analysis With Federal Students With Disabilities Data Learning objectives: Longitudinal analysis uses Importing data options Processing considerations: what order to take, what to consider for NA‚Äôs Importance of visualizing data Considering next steps for modeling analysis "],["slides-7.html", "8.1 Slides", " 8.1 Slides "],["meeting-videos-7.html", "8.2 Meeting Videos", " 8.2 Meeting Videos 8.2.1 Cohort 1 Meeting chat log 00:41:00 edgar zamora: https://stackoverflow.com/questions/4862178/remove-rows-with-all-or-some-nas-missing-values-in-data-frame 00:41:14 Mike Haugen: For time series analysis, some of the forecasting functions, e..g exponential smoothing, require a certain approach to dealing with NAs. You can remove NAs for some, for others, you need to impute them 00:42:20 Arami: Can you explain what &quot;time series analysis&quot; is? Is it any analysis that tracks change over time? 00:43:09 Mike Haugen: Yes 00:43:48 Mike Haugen: Like forecasting emergency room presentations based on historical data on emergency room presentations over the last few years 00:44:07 Mike Haugen: or forecasting course attendance based on historical data. 00:45:18 Rob Lucas: Glad to know there is some other list-aversion out there! 00:45:27 Mike Haugen: For R, see Hyndman Forecasting: Principles and Practice: https://otexts.com/fpp3/index.html 00:45:46 Arami: Thanks! 00:45:47 Ronak Patel: I also suffer from severe list-aversion. 00:46:33 Morgan Grovenburg: injuries %&gt;% mutate(diag = fct_lump(fct_infreq(diag), n = 5)) %&gt;% group_by(diag) %&gt;% summarise(n = as.integer(sum(weight))) "],["walkthrough-5-text-analysis-with-social-media-data.html", "Chapter 9 Walkthrough 5: Text Analysis With Social Media Data", " Chapter 9 Walkthrough 5: Text Analysis With Social Media Data Learning objectives: Understand how to retrieve data from Twitter Understand the robustness of Twitter data Learn basic principles of Natural Language Processing (NLP) Learn how to apply those principles to process social media data Learn how to use the tidytext package to analyze social media data Understand what a sentiment analysis is Learn how to perform a sentiment analysis on Twitter data "],["slides-8.html", "9.1 Slides", " 9.1 Slides "],["meeting-videos-8.html", "9.2 Meeting Videos", " 9.2 Meeting Videos 9.2.1 Cohort 1 TBD Meeting chat log 00:07:35 Layla Bouzoubaa: Lol I love the office band, I have it too!! 00:07:58 Ronak Patel: Always good for a chuckle at the start of a meeting! 00:32:49 Morgan Grovenburg: If you haven&#39;t used `tidytext` before, I recommend Julia Silge&#39;s free learnr course: https://juliasilge.com/blog/learn-tidytext-learnr/ 00:33:18 Alyssa Ibarra: Thanks! Is it just using whitespace as delimiter? 00:33:40 shamsuddeen: Also the book : https://www.tidytextmining.com 00:43:08 Mike Haugen: Love the Tidy Text book. Julia Silge also has Supervised Machine Learning for Text Analysis in R: https://smltar.com/ 01:02:30 Alyssa Ibarra: Thank you! 01:03:46 shamsuddeen: Good talk 01:04:35 Louis Carlo Medina: thank you!! "]]
